{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 09:08:46.490800: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-24 09:08:46.794357: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-24 09:08:46.794427: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-24 09:08:46.845939: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-24 09:08:46.927353: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-24 09:08:48.821651: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 09:08:51.029033: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 09:08:51.140324: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 09:08:51.146145: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "from typing import Any\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 09:09:02.970173: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 09:09:02.982434: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 09:09:02.990264: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 09:09:03.330787: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 09:09:03.332493: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 09:09:03.334315: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 09:09:03.335997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3987 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self,\n",
    "                 path,\n",
    "                 crop_size=128,\n",
    "                 rho=16):\n",
    "        self.crop_size = crop_size\n",
    "        self.rho = rho\n",
    "\n",
    "        # find all images in the path\n",
    "        if isinstance(path,str):\n",
    "            path = Path(path)\n",
    "        self.im_list = list(path.glob(\"*.jpg\"))\n",
    "        self.nimg = len(self.im_list)\n",
    "        shuffle(self.im_list)\n",
    "        self.ifile=0\n",
    "\n",
    "    def gen_img_and_homography(self,img_path,debug=False):\n",
    "    \n",
    "        # read image\n",
    "        im_data = np.array(Image.open(img_path))\n",
    "        if im_data.ndim < 3: # skip single channel\n",
    "            return None, None\n",
    "        h,w = im_data.shape[:2]\n",
    "\n",
    "        # randomly select the corners for cropping\n",
    "        # making sure the shifted corners are within the image\n",
    "        if isinstance(self.crop_size,int):\n",
    "            ch = self.crop_size\n",
    "            cw = self.crop_size\n",
    "        else: # list of crop size\n",
    "            ch,cw = self.crop_size\n",
    "\n",
    "        if h-self.rho-ch <= self.rho or w-self.rho-cw <= self.rho:\n",
    "            return None, None\n",
    "\n",
    "        # choose upper left corner\n",
    "        upper_left_h = np.random.randint(low=self.rho,high=h-self.rho-ch)\n",
    "        upper_left_w = np.random.randint(low=self.rho,high=w-self.rho-cw)\n",
    "\n",
    "        corner_pts = np.array([[0,0],[ch-1,0],[ch-1,cw-1],[0,cw-1]]) + \\\n",
    "                    np.array([upper_left_h, upper_left_w])[np.newaxis,:]\n",
    "        corner_pts_new = np.copy(corner_pts)\n",
    "        # generate the new 4 corner points\n",
    "        for i in range(4):\n",
    "            corner_pts_new[i,:] += np.random.randint(-self.rho,\n",
    "                                                     self.rho+1,\n",
    "                                                     size=(2,))\n",
    "        # note H map from corners new back to original corners\n",
    "\n",
    "        # calculate the difference in xy coordinate\n",
    "        H4pt = (corner_pts - corner_pts_new).flatten()\n",
    "\n",
    "        # get actual homography\n",
    "        H = cv2.getPerspectiveTransform(np.fliplr(corner_pts_new.astype(np.float32)),\n",
    "                                    np.fliplr(corner_pts.astype(np.float32)))\n",
    "        \n",
    "        im_warp = cv2.warpPerspective(im_data, H, (w,h))\n",
    "\n",
    "        # get the two patches\n",
    "        p1 = im_data[upper_left_h:upper_left_h+ch,upper_left_w:upper_left_w+cw]\n",
    "        p2 = im_warp[upper_left_h:upper_left_h+ch,upper_left_w:upper_left_w+cw]\n",
    "\n",
    "        p1 = tf.convert_to_tensor(p1, dtype=tf.float32)\n",
    "        p2 = tf.convert_to_tensor(p2, dtype=tf.float32)\n",
    "\n",
    "        # p1 = p1/255.0\n",
    "        # p2 = p2/255.0\n",
    "\n",
    "        if debug:\n",
    "            # sanity check, check the coordinate of the corners\n",
    "            for i in range(4):\n",
    "                tmp = H.dot(np.array([corner_pts_new[i,1],\n",
    "                                            corner_pts_new[i,0],\n",
    "                                            1.0])[:,np.newaxis])\n",
    "                print(corner_pts[i,:],np.flipud(tmp[:2]).flatten()/tmp[2])\n",
    "            plt.imshow(np.hstack((p1/255, p2/255)))\n",
    "\n",
    "        input = (p1,p2)\n",
    "        output = tf.convert_to_tensor(H4pt,dtype=tf.float32)\n",
    "        return input, output\n",
    "\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "        while True:\n",
    "            im_path = self.im_list[self.ifile]\n",
    "            self.ifile = (self.ifile+1)%self.nimg\n",
    "            im_crop, h4pt = self.gen_img_and_homography(im_path)\n",
    "            if im_crop is None:\n",
    "                continue\n",
    "            yield im_crop, h4pt\n",
    "\n",
    "# training set data generator\n",
    "train_path = Path(\"..\") / \"Phase2\" / \"Data\" / \"Train\"\n",
    "train_gen = DataGenerator(train_path)\n",
    "\n",
    "# tensorflow dataset\n",
    "im_shape = (train_gen.crop_size,train_gen.crop_size,3)\n",
    "output_signature = ((tf.TensorSpec(shape=im_shape,dtype=tf.float32),\n",
    "                     tf.TensorSpec(shape=im_shape,dtype=tf.float32)),\n",
    "                     tf.TensorSpec(shape = (8,),dtype=tf.float32),\n",
    "                    )\n",
    "train_ds = tf.data.Dataset.from_generator(train_gen,\n",
    "                                          output_signature=output_signature)\n",
    "\n",
    "def config_ds(ds):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    ds = ds.batch(32)\n",
    "    return ds\n",
    "\n",
    "train_ds = config_ds(train_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 128, 128, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 128, 128, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 64, 64, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 64, 64, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 32, 32, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 32, 32, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 32, 32, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 32, 32, 256)       590080    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2325568 (8.87 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 2325568 (8.87 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "\n",
    "base = VGG19(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(128,128,3),\n",
    "    pooling='max'\n",
    ")\n",
    "base.trainable = False\n",
    "\n",
    "# retrieve only some part of the base\n",
    "input = base.input\n",
    "output = base.get_layer('block3_conv4').output\n",
    "\n",
    "base2 = tf.keras.Model(inputs=input, outputs=output)\n",
    "base2.trainable = False\n",
    "base2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (None, 128, 128, 3)          0         ['input_2[0][0]']             \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1  (None, 128, 128, 3)          0         ['input_3[0][0]']             \n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " tf.nn.bias_add (TFOpLambda  (None, 128, 128, 3)          0         ['tf.__operators__.getitem[0][\n",
      " )                                                                  0]']                          \n",
      "                                                                                                  \n",
      " tf.nn.bias_add_1 (TFOpLamb  (None, 128, 128, 3)          0         ['tf.__operators__.getitem_1[0\n",
      " da)                                                                ][0]']                        \n",
      "                                                                                                  \n",
      " model (Functional)          (None, 32, 32, 256)          2325568   ['tf.nn.bias_add[0][0]',      \n",
      "                                                                     'tf.nn.bias_add_1[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 32, 32, 512)          0         ['model[0][0]',               \n",
      "                                                                     'model[1][0]']               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 32, 32, 256)          1179904   ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 32, 32, 256)          1024      ['conv2d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 256)          590080    ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 32, 32, 256)          1024      ['conv2d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 256)          590080    ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 32, 32, 256)          1024      ['conv2d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 256)          590080    ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 32, 32, 256)          1024      ['conv2d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 16, 16, 256)          590080    ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 16, 16, 256)          1024      ['conv2d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 256)            590080    ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 8, 8, 256)            1024      ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 16384)                0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 16384)                65536     ['flatten[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 512)                  8389120   ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 512)                  2048      ['dense[0][0]']               \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 512)                  262656    ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 512)                  2048      ['dense_1[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 8)                    4104      ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15187528 (57.94 MB)\n",
      "Trainable params: 12824072 (48.92 MB)\n",
      "Non-trainable params: 2363456 (9.02 MB)\n",
      "__________________________________________________________________________________________________\n",
      "input_2 True\n",
      "input_3 True\n",
      "tf.__operators__.getitem True\n",
      "tf.__operators__.getitem_1 True\n",
      "tf.nn.bias_add True\n",
      "tf.nn.bias_add_1 True\n",
      "model False\n",
      "concatenate True\n",
      "conv2d True\n",
      "batch_normalization True\n",
      "conv2d_1 True\n",
      "batch_normalization_1 True\n",
      "conv2d_2 True\n",
      "batch_normalization_2 True\n",
      "conv2d_3 True\n",
      "batch_normalization_3 True\n",
      "conv2d_4 True\n",
      "batch_normalization_4 True\n",
      "conv2d_5 True\n",
      "batch_normalization_5 True\n",
      "flatten True\n",
      "batch_normalization_6 True\n",
      "dense True\n",
      "batch_normalization_7 True\n",
      "dense_1 True\n",
      "batch_normalization_8 True\n",
      "dense_2 True\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import BatchNormalization as BN\n",
    "\n",
    "im_shape = (128,128,3)\n",
    "input1 = tf.keras.layers.Input(shape=im_shape)\n",
    "input2 = tf.keras.layers.Input(shape=im_shape)\n",
    "\n",
    "x1 = preprocess_input(input1)\n",
    "x2 = preprocess_input(input2)\n",
    "\n",
    "x1 = base2(x1)\n",
    "x2 = base2(x2)\n",
    "\n",
    "x = tf.keras.layers.Concatenate()((x1,x2)) # 32 by 32 by 512\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters=256,\n",
    "                            kernel_size=(3,3),\n",
    "                            activation='relu',\n",
    "                            padding='same')(x)\n",
    "x = BN()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters=256,\n",
    "                            kernel_size=(3,3),\n",
    "                            activation='relu',\n",
    "                            padding='same')(x)\n",
    "x = BN()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters=256,\n",
    "                            kernel_size=(3,3),\n",
    "                            activation='relu',\n",
    "                            padding='same')(x)\n",
    "x = BN()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters=256,\n",
    "                            kernel_size=(3,3),\n",
    "                            activation='relu',\n",
    "                            padding='same')(x)\n",
    "x = BN()(x)\n",
    "\n",
    "# add some 3x3 conv\n",
    "x = tf.keras.layers.Conv2D(filters=256,\n",
    "                            kernel_size=(3,3),\n",
    "                            strides=(2,2),\n",
    "                            activation='relu',\n",
    "                            padding='same')(x)\n",
    "\n",
    "x = BN()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters=256,\n",
    "                            kernel_size=(3,3),\n",
    "                            strides=(2,2),\n",
    "                            activation='relu',\n",
    "                            padding='same')(x)\n",
    "\n",
    "x = BN()(x)\n",
    "# # add 1x1 conv\n",
    "# x = tf.keras.layers.Conv2D(filters=256,\n",
    "#                             kernel_size=(1,1),\n",
    "#                             activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = BN()(x)\n",
    "# x = tf.keras.layers.LayerNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(units=512, activation='relu')(x)\n",
    "x = BN()(x)\n",
    "x = tf.keras.layers.Dense(units=512, activation='relu')(x)\n",
    "x = BN()(x)\n",
    "output = tf.keras.layers.Dense(units=8,activation=None)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input1,input2],outputs=output)\n",
    "model.summary()\n",
    "\n",
    "for l in model.layers:\n",
    "    print(l.name, l.trainable)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.norm(y_true-y_pred,axis=1))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=custom_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 09:10:39.915147: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-02-24 09:10:40.072874: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-02-24 09:10:41.527930: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-02-24 09:10:41.889763: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fcff429ace0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-24 09:10:41.889794: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2024-02-24 09:10:41.897544: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708783841.962521   12843 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 42s 232ms/step - loss: 25.5087 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "150/150 [==============================] - 32s 212ms/step - loss: 21.6179 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "150/150 [==============================] - 31s 210ms/step - loss: 18.9446 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "150/150 [==============================] - 31s 209ms/step - loss: 16.3879 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "150/150 [==============================] - 32s 211ms/step - loss: 14.6101 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "150/150 [==============================] - 32s 211ms/step - loss: 13.1867 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "150/150 [==============================] - 32s 211ms/step - loss: 12.5247 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "150/150 [==============================] - 32s 210ms/step - loss: 11.5966 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "150/150 [==============================] - 32s 213ms/step - loss: 11.1050 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "150/150 [==============================] - 32s 213ms/step - loss: 10.7191 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "150/150 [==============================] - 32s 213ms/step - loss: 10.2941 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "150/150 [==============================] - 32s 212ms/step - loss: 9.9171 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "150/150 [==============================] - 32s 211ms/step - loss: 9.6676 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "150/150 [==============================] - 32s 211ms/step - loss: 9.3751 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "150/150 [==============================] - 32s 212ms/step - loss: 9.2302 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "150/150 [==============================] - 32s 212ms/step - loss: 9.2353 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "150/150 [==============================] - 32s 212ms/step - loss: 9.0851 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "150/150 [==============================] - 32s 213ms/step - loss: 8.9353 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "150/150 [==============================] - 32s 211ms/step - loss: 8.8381 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "150/150 [==============================] - 32s 211ms/step - loss: 8.6873 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "150/150 [==============================] - 32s 214ms/step - loss: 8.6682 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "150/150 [==============================] - 32s 214ms/step - loss: 8.4613 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "150/150 [==============================] - 32s 213ms/step - loss: 8.4303 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "150/150 [==============================] - 32s 215ms/step - loss: 8.4560 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "150/150 [==============================] - 32s 212ms/step - loss: 8.2379 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "150/150 [==============================] - 32s 215ms/step - loss: 8.1815 - lr: 0.0010\n",
      "Epoch 27/150\n",
      "150/150 [==============================] - 32s 214ms/step - loss: 8.1776 - lr: 0.0010\n",
      "Epoch 28/150\n",
      "150/150 [==============================] - 32s 216ms/step - loss: 8.1673 - lr: 0.0010\n",
      "Epoch 29/150\n",
      "150/150 [==============================] - 31s 210ms/step - loss: 7.9602 - lr: 0.0010\n",
      "Epoch 30/150\n",
      "150/150 [==============================] - 32s 214ms/step - loss: 7.9104 - lr: 0.0010\n",
      "Epoch 31/150\n",
      "150/150 [==============================] - 32s 213ms/step - loss: 7.8855 - lr: 0.0010\n",
      "Epoch 32/150\n",
      "150/150 [==============================] - 32s 213ms/step - loss: 7.8333 - lr: 0.0010\n",
      "Epoch 33/150\n",
      "150/150 [==============================] - 32s 211ms/step - loss: 7.7388 - lr: 0.0010\n",
      "Epoch 34/150\n",
      "150/150 [==============================] - 31s 204ms/step - loss: 7.7588 - lr: 0.0010\n",
      "Epoch 35/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 7.6256 - lr: 0.0010\n",
      "Epoch 36/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 7.4859 - lr: 0.0010\n",
      "Epoch 37/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 7.4985 - lr: 0.0010\n",
      "Epoch 38/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 7.6449 - lr: 0.0010\n",
      "Epoch 39/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 7.5722 - lr: 0.0010\n",
      "Epoch 40/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 7.5962 - lr: 0.0010\n",
      "Epoch 41/150\n",
      "150/150 [==============================] - 30s 199ms/step - loss: 7.4408 - lr: 0.0010\n",
      "Epoch 42/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 7.3764 - lr: 0.0010\n",
      "Epoch 43/150\n",
      "150/150 [==============================] - 30s 199ms/step - loss: 7.4819 - lr: 0.0010\n",
      "Epoch 44/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 7.3764 - lr: 0.0010\n",
      "Epoch 45/150\n",
      "150/150 [==============================] - 30s 198ms/step - loss: 7.2706 - lr: 0.0010\n",
      "Epoch 46/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 7.1775 - lr: 0.0010\n",
      "Epoch 47/150\n",
      "150/150 [==============================] - 30s 199ms/step - loss: 7.0675 - lr: 0.0010\n",
      "Epoch 48/150\n",
      "150/150 [==============================] - 30s 199ms/step - loss: 7.1166 - lr: 0.0010\n",
      "Epoch 49/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 7.2076 - lr: 0.0010\n",
      "Epoch 50/150\n",
      "150/150 [==============================] - 30s 198ms/step - loss: 7.1521 - lr: 0.0010\n",
      "Epoch 51/150\n",
      "150/150 [==============================] - 30s 202ms/step - loss: 7.1407 - lr: 0.0010\n",
      "Epoch 52/150\n",
      "150/150 [==============================] - ETA: 0s - loss: 7.1869\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 7.1869 - lr: 0.0010\n",
      "Epoch 53/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.6887 - lr: 2.0000e-04\n",
      "Epoch 54/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.5687 - lr: 2.0000e-04\n",
      "Epoch 55/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.5441 - lr: 2.0000e-04\n",
      "Epoch 56/150\n",
      "150/150 [==============================] - 30s 202ms/step - loss: 6.5058 - lr: 2.0000e-04\n",
      "Epoch 57/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.5465 - lr: 2.0000e-04\n",
      "Epoch 58/150\n",
      "150/150 [==============================] - 30s 202ms/step - loss: 6.4461 - lr: 2.0000e-04\n",
      "Epoch 59/150\n",
      "150/150 [==============================] - 30s 199ms/step - loss: 6.4572 - lr: 2.0000e-04\n",
      "Epoch 60/150\n",
      "150/150 [==============================] - 30s 199ms/step - loss: 6.4643 - lr: 2.0000e-04\n",
      "Epoch 61/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.4971 - lr: 2.0000e-04\n",
      "Epoch 62/150\n",
      "150/150 [==============================] - 30s 203ms/step - loss: 6.4065 - lr: 2.0000e-04\n",
      "Epoch 63/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.4272 - lr: 2.0000e-04\n",
      "Epoch 64/150\n",
      "150/150 [==============================] - 30s 199ms/step - loss: 6.3862 - lr: 2.0000e-04\n",
      "Epoch 65/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.4965 - lr: 2.0000e-04\n",
      "Epoch 66/150\n",
      "150/150 [==============================] - 30s 203ms/step - loss: 6.3413 - lr: 2.0000e-04\n",
      "Epoch 67/150\n",
      "150/150 [==============================] - 31s 203ms/step - loss: 6.5046 - lr: 2.0000e-04\n",
      "Epoch 68/150\n",
      "150/150 [==============================] - 30s 199ms/step - loss: 6.3781 - lr: 2.0000e-04\n",
      "Epoch 69/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.3862 - lr: 2.0000e-04\n",
      "Epoch 70/150\n",
      "150/150 [==============================] - 30s 202ms/step - loss: 6.3656 - lr: 2.0000e-04\n",
      "Epoch 71/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.2899 - lr: 2.0000e-04\n",
      "Epoch 72/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.3049 - lr: 2.0000e-04\n",
      "Epoch 73/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.2372 - lr: 2.0000e-04\n",
      "Epoch 74/150\n",
      "150/150 [==============================] - 30s 202ms/step - loss: 6.3109 - lr: 2.0000e-04\n",
      "Epoch 75/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.3214 - lr: 2.0000e-04\n",
      "Epoch 76/150\n",
      "150/150 [==============================] - 30s 199ms/step - loss: 6.1890 - lr: 2.0000e-04\n",
      "Epoch 77/150\n",
      "150/150 [==============================] - 30s 202ms/step - loss: 6.2300 - lr: 2.0000e-04\n",
      "Epoch 78/150\n",
      "150/150 [==============================] - 30s 202ms/step - loss: 6.1533 - lr: 2.0000e-04\n",
      "Epoch 79/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.2449 - lr: 2.0000e-04\n",
      "Epoch 80/150\n",
      "150/150 [==============================] - 30s 199ms/step - loss: 6.2865 - lr: 2.0000e-04\n",
      "Epoch 81/150\n",
      "150/150 [==============================] - 30s 202ms/step - loss: 6.1117 - lr: 2.0000e-04\n",
      "Epoch 82/150\n",
      "150/150 [==============================] - 30s 202ms/step - loss: 6.2730 - lr: 2.0000e-04\n",
      "Epoch 83/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.1985 - lr: 2.0000e-04\n",
      "Epoch 84/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.2941 - lr: 2.0000e-04\n",
      "Epoch 85/150\n",
      "150/150 [==============================] - 30s 198ms/step - loss: 6.1388 - lr: 2.0000e-04\n",
      "Epoch 86/150\n",
      "150/150 [==============================] - 30s 198ms/step - loss: 6.0400 - lr: 2.0000e-04\n",
      "Epoch 87/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.2666 - lr: 2.0000e-04\n",
      "Epoch 88/150\n",
      "150/150 [==============================] - 30s 198ms/step - loss: 6.1811 - lr: 2.0000e-04\n",
      "Epoch 89/150\n",
      "150/150 [==============================] - 30s 203ms/step - loss: 6.2141 - lr: 2.0000e-04\n",
      "Epoch 90/150\n",
      "150/150 [==============================] - 31s 204ms/step - loss: 6.1527 - lr: 2.0000e-04\n",
      "Epoch 91/150\n",
      "150/150 [==============================] - ETA: 0s - loss: 6.1517\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.1517 - lr: 2.0000e-04\n",
      "Epoch 92/150\n",
      "150/150 [==============================] - 30s 202ms/step - loss: 5.9612 - lr: 4.0000e-05\n",
      "Epoch 93/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.0364 - lr: 4.0000e-05\n",
      "Epoch 94/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.0053 - lr: 4.0000e-05\n",
      "Epoch 95/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.0812 - lr: 4.0000e-05\n",
      "Epoch 96/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 5.9142 - lr: 4.0000e-05\n",
      "Epoch 97/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.1105 - lr: 4.0000e-05\n",
      "Epoch 98/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 5.8914 - lr: 4.0000e-05\n",
      "Epoch 99/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 5.9869 - lr: 4.0000e-05\n",
      "Epoch 100/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.0644 - lr: 4.0000e-05\n",
      "Epoch 101/150\n",
      "150/150 [==============================] - 30s 198ms/step - loss: 5.9915 - lr: 4.0000e-05\n",
      "Epoch 102/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.0344 - lr: 4.0000e-05\n",
      "Epoch 103/150\n",
      "150/150 [==============================] - ETA: 0s - loss: 6.0085\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.0085 - lr: 4.0000e-05\n",
      "Epoch 104/150\n",
      "150/150 [==============================] - 30s 202ms/step - loss: 6.1208 - lr: 8.0000e-06\n",
      "Epoch 105/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.1053 - lr: 8.0000e-06\n",
      "Epoch 106/150\n",
      "150/150 [==============================] - 30s 202ms/step - loss: 5.9801 - lr: 8.0000e-06\n",
      "Epoch 107/150\n",
      "150/150 [==============================] - 30s 203ms/step - loss: 5.9342 - lr: 8.0000e-06\n",
      "Epoch 108/150\n",
      "150/150 [==============================] - 30s 203ms/step - loss: 6.0342 - lr: 8.0000e-06\n",
      "Epoch 109/150\n",
      "150/150 [==============================] - 30s 197ms/step - loss: 6.1094 - lr: 8.0000e-06\n",
      "Epoch 110/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 5.9998 - lr: 8.0000e-06\n",
      "Epoch 111/150\n",
      "150/150 [==============================] - 30s 202ms/step - loss: 6.0717 - lr: 8.0000e-06\n",
      "Epoch 112/150\n",
      "150/150 [==============================] - ETA: 0s - loss: 5.9274\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 5.9274 - lr: 8.0000e-06\n",
      "Epoch 113/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.0528 - lr: 1.6000e-06\n",
      "Epoch 114/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 5.9017 - lr: 1.6000e-06\n",
      "Epoch 115/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.0597 - lr: 1.6000e-06\n",
      "Epoch 116/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.0443 - lr: 1.6000e-06\n",
      "Epoch 117/150\n",
      "150/150 [==============================] - 30s 199ms/step - loss: 5.9557 - lr: 1.6000e-06\n",
      "Epoch 118/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.0251 - lr: 1.6000e-06\n",
      "Epoch 119/150\n",
      "150/150 [==============================] - 30s 203ms/step - loss: 6.0195 - lr: 1.6000e-06\n",
      "Epoch 120/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.0310 - lr: 1.6000e-06\n",
      "Epoch 121/150\n",
      "150/150 [==============================] - ETA: 0s - loss: 6.0025\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.0025 - lr: 1.6000e-06\n",
      "Epoch 122/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.0032 - lr: 1.0000e-06\n",
      "Epoch 123/150\n",
      "150/150 [==============================] - 30s 199ms/step - loss: 5.9639 - lr: 1.0000e-06\n",
      "Epoch 124/150\n",
      "150/150 [==============================] - 30s 203ms/step - loss: 6.0924 - lr: 1.0000e-06\n",
      "Epoch 125/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 5.9791 - lr: 1.0000e-06\n",
      "Epoch 126/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 5.9896 - lr: 1.0000e-06\n",
      "Epoch 127/150\n",
      "150/150 [==============================] - 30s 199ms/step - loss: 6.1458 - lr: 1.0000e-06\n",
      "Epoch 128/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.0193 - lr: 1.0000e-06\n",
      "Epoch 129/150\n",
      "150/150 [==============================] - 30s 202ms/step - loss: 6.0543 - lr: 1.0000e-06\n",
      "Epoch 130/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 5.9551 - lr: 1.0000e-06\n",
      "Epoch 131/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.0511 - lr: 1.0000e-06\n",
      "Epoch 132/150\n",
      "150/150 [==============================] - 31s 204ms/step - loss: 6.1365 - lr: 1.0000e-06\n",
      "Epoch 133/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.0556 - lr: 1.0000e-06\n",
      "Epoch 134/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 5.8789 - lr: 1.0000e-06\n",
      "Epoch 135/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 5.9906 - lr: 1.0000e-06\n",
      "Epoch 136/150\n",
      "150/150 [==============================] - 30s 203ms/step - loss: 6.0533 - lr: 1.0000e-06\n",
      "Epoch 137/150\n",
      "150/150 [==============================] - 30s 199ms/step - loss: 5.9843 - lr: 1.0000e-06\n",
      "Epoch 138/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.0314 - lr: 1.0000e-06\n",
      "Epoch 139/150\n",
      "150/150 [==============================] - 30s 199ms/step - loss: 6.0747 - lr: 1.0000e-06\n",
      "Epoch 140/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 5.9477 - lr: 1.0000e-06\n",
      "Epoch 141/150\n",
      "150/150 [==============================] - 30s 203ms/step - loss: 6.0100 - lr: 1.0000e-06\n",
      "Epoch 142/150\n",
      "150/150 [==============================] - 30s 203ms/step - loss: 5.8552 - lr: 1.0000e-06\n",
      "Epoch 143/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 6.0579 - lr: 1.0000e-06\n",
      "Epoch 144/150\n",
      "150/150 [==============================] - 30s 200ms/step - loss: 5.9797 - lr: 1.0000e-06\n",
      "Epoch 145/150\n",
      "150/150 [==============================] - 30s 202ms/step - loss: 5.9482 - lr: 1.0000e-06\n",
      "Epoch 146/150\n",
      "150/150 [==============================] - 30s 199ms/step - loss: 6.0649 - lr: 1.0000e-06\n",
      "Epoch 147/150\n",
      "150/150 [==============================] - 30s 202ms/step - loss: 6.0308 - lr: 1.0000e-06\n",
      "Epoch 148/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.0207 - lr: 1.0000e-06\n",
      "Epoch 149/150\n",
      "150/150 [==============================] - 30s 203ms/step - loss: 6.0559 - lr: 1.0000e-06\n",
      "Epoch 150/150\n",
      "150/150 [==============================] - 30s 201ms/step - loss: 6.1553 - lr: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
    "                                                factor=0.2,\n",
    "                                                patience=5,\n",
    "                                                min_lr=1e-6,\n",
    "                                                verbose=1,\n",
    "                                                cooldown=5)\n",
    "\n",
    "history = model.fit(train_ds,epochs=150,verbose=True,steps_per_epoch=150,\n",
    "                    callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd094126990>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmsc818",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
